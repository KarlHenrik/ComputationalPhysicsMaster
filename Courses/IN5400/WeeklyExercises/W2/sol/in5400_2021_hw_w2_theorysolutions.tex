%%%%%%%%%%%%%% HEADER: START %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}

%\geometry{showframe}% for debugging purposes -- displays the margins

\usepackage{amsmath,amssymb}
\usepackage{verbatim}
\usepackage{url}

%\include{styleV5}%-print}

% Set up the images/graphics package
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

\title{IN5400 2021 UiO -- exercise tasks }

\author{reader: Alexander Binder, exercise mentor: Gabriel Balaban }
\date{Week 02: Pytorch for a start} % if the \date{} command is left out, the current date will be used
%%%%%%%%%%%%%% HEADER: END %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% PACKAGES ETC: START %%%%%%%%%%%%%%%%%%%%%%%%%
% The following package makes prettier tables.  We're all about the bling!
\usepackage{booktabs}

% The units package provides nice, non-stacked fractions and better spacing
% for units.
\usepackage{units}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

% Small sections of multiple columns
\usepackage{multicol}

% Provides paragraphs of dummy text
\usepackage{lipsum}

% These commands are used to pretty-print LaTeX commands
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name


\newcommand{\eqn}[1]{\begin{align*}#1\end{align*}}% document class option name
\newcommand{\ben}[1]{\begin{enumerate}#1\end{enumerate}}
\newcommand{\bi}[1]{\begin{itemize}#1\end{itemize}}
\newcommand{\eps}{\epsilon}
\newcommand{\mc}[1]{\mathcal{#1}}

\newcommand{\drf}[2]{ \frac{\partial #1}{\partial #2} }
\newcommand{\s}{ \\\vspace{3mm} }
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\RR}{\mathbb{R}}

\usepackage{bm}

\usepackage{color}	
%\usepackage{fancyhdr}
%\pagestyle{fancy}

\usepackage{etoolbox}
%%%%%%%%%%%%%% PACKAGES ETC: END %%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% DEFINE CONDITIONALS: START %%%%%%%%%%%%%%%%%%%%%%%%%
% The following if statements are used to generate three different versions of the notes


\newtoggle{lecture}
\toggletrue{lecture}



 % Uncomment to include footnotes

%%%%%%%%%%%%%% DEFINE CONDITIONALS: END %%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle % this prints the handout title, author, and date

%\begin{abstract}
% Abstract can be included if needed
%\end{abstract}
\vspace{0.5cm}



\setlength\parindent{0pt}
 %%%% HEEEERE

%The goal is to learn that pytorch can be used for more than just neural networks.

\section{Task1}

If you need a recap on numpy-indexing, then do the tasks in the files {\tt indexing.ipynb} and {\tt math\_operations.ipynb}.

\section{Task2 -- Broadcasting}

Which of these shapes are compatible under broadcasting for a simple binary operator like addition or multiplication? If they are, what is the resulting shape

\bi{
\item $(3,1,3), (2,3,3)$ -- \textcolor{red}{ERR}
\item $(4,1), (3,1,1,5)$ -- $(3,1,4,5)$
\item $(3), (3,1,1,5)$ -- \textcolor{red}{ERR}
\item $(1,4),(7,1)$ -- $(7,4)$
\item $(6,3,1,7),(2,7)$ -- $(6,3,2,7)$
\item $(6,3,1,7),(2,1,7)$ -- \textcolor{red}{ERR}
\item $(1,5,3,1,6),(8,1,3,2,6)$ -- $(8,5,3,2,6)$ 
\item $(2,5,1,7),(9,2,3,2,1)$ -- \textcolor{red}{ERR}
}

\section{Task3}


The goal is to let you see the power of broadcasting for speeding up computations. Also to see that you can use pytorch with GPU to speed up any other computations than vanilla deep learning. All it needs is that they can be expressed by linear algebra.\s

You may have seen in a machine learning lecture from your past the RBF kernel which is a matrix
\eqn{
\phi(x_i,t_j) = \exp(- \frac{\|X[i,:]-T[j,:] \|^2 }{\gamma} )
}
Inside is a squared $\ell_2$-distance matrix between $X[i,:]$ and $T[j,:]$.

\eqn{
X.size()=(N,D)\\
T.size()=(P,D)
}

$X$ are features with dimensionality $D$ and sample size $N$. $T$ are prototypes with dimensionality $D$ and sample size $P$. Use pytorch to compute the distance 
\eqn{
 \|X[i,:]-T[j,:] \|^2
}
which is underlying the RBF kernel.\s

Problem today: write code to compute a matrix $ d_{ij}:= \|X(i,:)-T(j,:)\|^2 $ in pytorch - \underline{without for loops}, using broadcasting. You will start with numpy and for loops, then implement the same in pytorch without for loops.

Approach:
\bi{
\item decompose above formula into a sequence of computations
\item how to reshape $X$, $T$ such that you can use broadcasting to get $d_{ij}=\|X(i,:)-T(j,:)\|^2$ ? There is more than one way, and these ways can differ in execution speed and mem usage.
\item find pytorch operations to compute that ...
}

Compare time measurements:
\bi{
\item two for-loops over $i,j$ (for $x_i,t_j$)
\item numpy broadcasting
\item pytorch cpu
\item optional: pytorch on a gpu (a cheap notebook gpu with 2Gbyte is good enough ) for $N,P,D$ nicely large
}
Validate that pytorch cpu gives you the same numerical result as one of the two: for loops or numpy broadcasting.\s

%Note if $T=X$, then this is a RBF kernel which is popular with SVMs for small data problems.




\section{Task4: Regarding inner products and angles, a matrix can be seen as just a vector.\\ Blue pill $=$ red pill.}

An inner product between two $D$-dim vectors $v,w$ is defined by 
\begin{align}
v \cdot w = \sum_{d=1}^D v_d w_d \label{eq:innerprod}
\end{align}

Consider two matrices $A,B \in \RR^{(m,l) }$. Define
\eqn{
A \cdot B &:= tr( A^\top B ) \label{eq:innerprod2} \\
\text{where } tr(Z) &:= \sum_i Z_{ii}
}

\bi{
\item Prove that $A \cdot B $ can be written as an inner product of two vectors as in Equation \eqref{eq:innerprod}.




\eqn{
tr( A^\top B )  = \sum_i ( A^\top B )_{ii} \\
 (A^\top B)_{ii} = \sum_k (A^\top)_{ik} B_{ki} = \sum_k A_{ki} B_{ki}\\
\Rightarrow tr( A^\top B )  = \sum_{i,k} A_{ki} B_{ki}\\
}
Now flatten the tensors:
$v(A) \in \RR^{m * l } $, $v(A)_{i+k*l} := A_{ki} $, then:
\eqn{
tr( A^\top B )  = \sum_{i,k} A_{ki} B_{ki} = \sum_{r=1}^{m*l} v(A)_r v(B)_r  = v(A) \cdot v(B)
}


\item Prove that $A \cdot B =  B \cdot A$. It is indeed symmetric even if it does not look like that.

\eqn{
tr( A^\top B )  =  \sum_{i,k} A_{ki} B_{ki} = \sum_{i,k} B_{ki} A_{ki} = tr( B^\top A )
}


\item We know that for every inner product it holds $v \cdot w = \|v\| \|w\| \cos \angle (v,w)  $. So what is the cosine angle between $ \begin{pmatrix} 1 & -2 \\ -2 & 3 \end{pmatrix}  $ and  $ \begin{pmatrix} -1 & 1 \\ 0 & -2 \end{pmatrix}  $ ? Which angles in $[0,2\pi]$ can give rise to the computed cosine angle?


the simplest way is to flatten them in any order
\eqn{
(1,-2,-2,3) \cdot (-1,1,0,-2) = -9\\
\|A\| = \sqrt{ 1+4+4+9 } = \sqrt{ 18 } \\
\|B\| = \sqrt{ 6 } = \sqrt{ 6 }\\
\cos \angle (A,B) =  \frac{-9}{ \sqrt{6*18} } = -\frac{3}{2 \sqrt{3}} \\
\angle (A,B) \in \{ \approx 2.6179, 2*\pi - 2.6179 \} + 2\pi l
}

}

Note: you can define analogously an inner product between any pairs of $n$-tensors:
\eqn{
A \cdot B = \sum_{k_1,k_2,\ldots,k_n} A[k_1,k_2,\ldots,k_n] B [k_1,k_2,\ldots,k_n]
}






\section{Task5 (coding Bonus)}

Now take what you got before from Task1 to create your own pytorch k-means algorithm.

How does k-means work?

\bi{
\item: given data $X[i,:]$ (create in 2-dimensions say 5 blobs by drawing data from 5 gaussians with different means and sufficiently small variance), and a desired number of clusters $P$ (for above dataset play with $P=2,5,8$), and a max number of iterations $M$. Do something like 50 datapoints per blob.

\item initialize cluster centers $T[j,:]$ -- for simplicity select your 5 cluster centers from $X[i,:]$ as fixed indices. A real k-means code would draw them randomly from the dataset, however the TA will need to be able to reproduce your code, so thats why we fix them.

\item then iterate in a for loop for at most $M times$:
\bi{
\item compute the distance matrix $\|X[i,:]-T[j,:] \|^2$ as per task1
\item for each sample $X[i,:]$ find the index $j$ of the cluster rep $T[j,:]$ which is nearest. pytorch has a function for that. Think over what dimension of the distance tensor the minimum needs to be taken!

\item Let $Ind(j) = \{  i: \ \forall r \neq j: \|X[i,:]-T[j,: \|^2 \le \|X[i,:]-T[r,:] \|^2    \}$ 
be the set of all those indices $i$ such that cluster rep $T[j,:]$ is their nearest one. Now recompute $T[j,:]$ as the mean of all those $X[i,:]$ for which $i \in Ind(j)$. do this for every cluster index $j$.
\item terminate if either the number of iterations reaches $M$, or if the distance between old $T[j,:]$ and recomputed $T[j,:]$ is for all $j$ below a threshold.
}
}


Visualize the data and the initial and the converged cluster centers by matplotlib (can be in 2 separate plots).\s

%Submit for homework both codes (Task1 and Task2), as well as the plots for k-means and the used dataset. make the k-means run in such a way for the submission code, that the initialization of clusters are fixed samples.\s

Note1: k-means is a non-convex algorithm. if you run it with random, not fixed initializations, then you will get not always the same clustering as result!\s

%Note2: i could have asked you to compute an RBF kernel over the imageclef features instead.



\end{document}
