{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "representative-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "from vocparseclslabels import PascalVOC\n",
    "\n",
    "from typing import Callable, Optional\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specialized-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for getting image tensors and labels of training and validation data\n",
    "class dataset_voc(Dataset):\n",
    "    def __init__(self, root_dir, trvaltest, transform=None):\n",
    "        self._labels = []\n",
    "        self._imgfilenames = []\n",
    "        self._root_dir = root_dir\n",
    "        self._transform = transform\n",
    "        \n",
    "        trvaltest = [\"train\", \"val\", \"test\"][trvaltest] # going from integer to string identifier\n",
    "        \n",
    "        pVOC = PascalVOC(root_dir) # object for getting the correct file names\n",
    "        im_dict = {}\n",
    "        for i, im_class in enumerate(pVOC.list_image_sets()):\n",
    "            files = pVOC.imgs_from_category_as_list(im_class, trvaltest) # all files with class im_class in tr/val/test data\n",
    "            for file in files:\n",
    "                if file in im_dict:\n",
    "                    im_dict[file][i] = 1\n",
    "                else:\n",
    "                    im_dict[file] = np.zeros(20)\n",
    "                    im_dict[file][i] = 1\n",
    "\n",
    "        for key in im_dict:\n",
    "            self._labels.append(im_dict[key])\n",
    "            self._imgfilenames.append(key)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._imgfilenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Variable(self._im2tensor(self._imgfilenames[idx]))\n",
    "        label = Variable(torch.tensor(self._labels[idx]).float())\n",
    "        sample = {\"image\": image, \"label\": label, \"filename\": self._imgfilenames[idx]}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def _im2tensor(self, im_name):\n",
    "        img_path = self._root_dir + \"JPEGImages/\" + im_name +\".jpg\"\n",
    "        image = Image.open(img_path)\n",
    "        if self._transform:\n",
    "            image = self._transform(image)\n",
    "        else:\n",
    "            transforms.ToTensor()(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function which is essentially a weighted binary cross entropy loss\n",
    "class BCE_custom(nn.modules.loss._Loss):\n",
    "    def __init__(self, reduction: str = \"mean\") -> None:\n",
    "        super(BCE_custom, self).__init__()\n",
    "        self._reduction = reduction\n",
    "        self._BCE = nn.BCELoss(reduction = 'none') # Binary cross entropy for calculating cross entropy of every class prediction\n",
    "\n",
    "    def forward(self, input_: Tensor, target: Tensor) -> Tensor:\n",
    "        BCE_vec = self._BCE(input_, target)\n",
    "        trues = torch.mean(BCE_vec[target.bool()]) # average loss where there should be a label\n",
    "        falses = torch.mean(BCE_vec[~target.bool()]) # average loss where there should not be a label\n",
    "        return trues + falses # this way guessing 0 for all is only 50% correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "correct-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_meanavgprecision(model, dataloader, loss_func, device, numcl):\n",
    "    model.eval() # Makes the forward pass more efficient for evaluation\n",
    "\n",
    "    curcount = 0\n",
    "    accuracy = 0\n",
    "    idx_start = 0\n",
    "    \n",
    "    num_items = len(dataloader.dataset)\n",
    "    pred_arr = torch.from_numpy(np.zeros(shape=(num_items, numcl))).to(device) # prediction scores for each class. each row is a list of scores. one score per image\n",
    "    label_arr = torch.from_numpy(np.zeros(shape=(num_items, numcl))).to(device) # labels scores for each class. each row is a list of labels. one label per image\n",
    "    fnames = []  # filenames as they come out of the dataloader\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad(): # We need no gradients for evaluation, so this is faster\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            if (batch_idx % 100 == 0) and (batch_idx >= 100):\n",
    "                print(f\"at val batchindex: {batch_idx}\")\n",
    "\n",
    "            # Extracting data\n",
    "            fname = data[\"filename\"]\n",
    "            inputs = data[\"image\"].to(device)\n",
    "            labels = data[\"label\"].to(device)\n",
    "            # Calculating what we need\n",
    "            pred = model(inputs.to(device))\n",
    "            loss = loss_func(pred, labels)\n",
    "            # Storing values\n",
    "            batch_size = len(fname)\n",
    "            idx_end = idx_start + batch_size\n",
    "            label_arr[idx_start:idx_end] = labels\n",
    "            pred_arr[idx_start:idx_end] = pred\n",
    "            idx_start += batch_size\n",
    "\n",
    "            fnames += fname # concatenating lists\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    avgprecs = np.zeros(numcl)  # average precision for each class\n",
    "    for c in range(numcl):\n",
    "        avgprecs[c] = sklearn.metrics.average_precision_score(y_true = label_arr[:, c].to(\"cpu\"), y_score = pred_arr[:, c].to(\"cpu\"))\n",
    "        \n",
    "    return avgprecs, np.mean(losses), label_arr, pred_arr, fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "celtic-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traineval2_model_nocv(dataloader_train, dataloader_test,  model, loss_func, optimizer, scheduler, num_epochs, device, numcl):\n",
    "    best_measure = 0\n",
    "    best_epoch = -1\n",
    "\n",
    "    trainlosses = []\n",
    "    testlosses = []\n",
    "    testperfs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"----------\")\n",
    "\n",
    "        avgloss = train_epoch(model, dataloader_train, loss_func, device, optimizer)\n",
    "        trainlosses.append(avgloss)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        perfmeasure, testloss, concat_labels, concat_pred, fnames = evaluate_meanavgprecision(\n",
    "            model, dataloader_test, loss_func, device, numcl)\n",
    "        testlosses.append(testloss)\n",
    "        testperfs.append(perfmeasure)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} \\n Classwise perfmeasure: {perfmeasure}\")\n",
    "\n",
    "        avgperfmeasure = np.mean(perfmeasure)\n",
    "        print(f\"Avgperfmeasure: {avgperfmeasure}, train {avgloss}, test {testloss}\")\n",
    "\n",
    "        if avgperfmeasure > best_measure:\n",
    "            bestweights = model.state_dict()\n",
    "            best_measure = avgperfmeasure\n",
    "            best_epoch = epoch\n",
    "\n",
    "    return best_epoch, best_measure, bestweights, trainlosses, testlosses, testperfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "renewable-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, trainloader, loss_func, device, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for batch_idx, data in enumerate(trainloader):\n",
    "        inputs = data[\"image\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stuck-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- SETUP BLOCK --------------------------\n",
    "torch.manual_seed(1)\n",
    "config = dict()\n",
    "\n",
    "# True #TODO change this to True for training on the cluster, eh\n",
    "config[\"use_gpu\"] = True\n",
    "config[\"lr\"] = 0.005\n",
    "config[\"batchsize_train\"] = 16\n",
    "config[\"batchsize_val\"] = 64\n",
    "config[\"maxnumepochs\"] = 35\n",
    "\n",
    "config[\"scheduler_stepsize\"] = 10\n",
    "config[\"scheduler_factor\"] = 0.3\n",
    "\n",
    "# kind of a dataset property\n",
    "config[\"numcl\"] = 20\n",
    "\n",
    "# data augmentations\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# datasets\n",
    "root_dir = \"data/VOCdevkit/VOC2012/\"\n",
    "image_datasets = {}\n",
    "image_datasets[\"train\"] = dataset_voc(root_dir = root_dir, trvaltest=0, transform=data_transforms[\"train\"])\n",
    "image_datasets[\"val\"] = dataset_voc(root_dir = root_dir, trvaltest=1, transform=data_transforms[\"val\"])\n",
    "\n",
    "# dataloaders\n",
    "dataloaders = {}\n",
    "dataloaders[\"train\"] = DataLoader(image_datasets[\"train\"], num_workers=0, batch_size = config[\"batchsize_train\"])\n",
    "dataloaders[\"val\"] = DataLoader(image_datasets[\"val\"], num_workers=0, batch_size = config[\"batchsize_val\"])\n",
    "\n",
    "# device\n",
    "if True == config[\"use_gpu\"]:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# model\n",
    "model = models.resnet18(pretrained=True) # pretrained resnet18\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Sequential( # rewriting last layer to have 20 sigmoid outputs\n",
    "    nn.Linear(512, 20),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "#loss_func = BCE_custom()\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "#optimizer = optim.Adam(params = model.fc.parameters(), lr = config[\"lr\"])\n",
    "optimizer = optim.SGD(params = model.fc.parameters(), lr = config[\"lr\"])\n",
    "\n",
    "# Decay LR by a factor of 0.3 every X epochs\n",
    "lr_sc = lr_scheduler.StepLR(optimizer, step_size = config[\"scheduler_stepsize\"], gamma = config[\"scheduler_factor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "maritime-review",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "----------\n",
      "Epoch: 1 \n",
      " Classwise perfmeasure: [0.06817351 0.16913744 0.06006747 0.05396996 0.05705205 0.02850473\n",
      " 0.09460633 0.06232385 0.06664164 0.04120203 0.0350299  0.09094914\n",
      " 0.04432153 0.0445771  0.45258757 0.03659378 0.01801119 0.04029494\n",
      " 0.04313908 0.04999609]\n",
      "Avgperfmeasure: 0.07785896670335432, train 0.27723027447332216, test 0.2554746129355588\n",
      "Epoch 2/35\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5dca1f47bb5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m best_epoch, best_measure, bestweights, trainlosses, testlosses, testperfs = traineval2_model_nocv(\n\u001b[1;32m----> 2\u001b[1;33m         dataloaders[\"train\"], dataloaders[\"val\"], model, loss_func, optimizer, lr_sc, num_epochs=config[\"maxnumepochs\"], device=device, numcl=config[\"numcl\"])\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-185b1f9ae53b>\u001b[0m in \u001b[0;36mtraineval2_model_nocv\u001b[1;34m(dataloader_train, dataloader_test, model, loss_func, optimizer, scheduler, num_epochs, device, numcl)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"----------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mavgloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mtrainlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavgloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-76cf43dfd158>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, trainloader, loss_func, device, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d08d5185da41>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_im2tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_imgfilenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"label\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"filename\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_imgfilenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d08d5185da41>\u001b[0m in \u001b[0;36m_im2tensor\u001b[1;34m(self, im_name)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_im2tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"JPEGImages/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mim_name\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2911\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2913\u001b[1;33m     \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2915\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_epoch, best_measure, bestweights, trainlosses, testlosses, testperfs = traineval2_model_nocv(\n",
    "        dataloaders[\"train\"], dataloaders[\"val\"], model, loss_func, optimizer, lr_sc, num_epochs=config[\"maxnumepochs\"], device=device, numcl=config[\"numcl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-magazine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
