{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import PIL.Image\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vocparseclslabels import PascalVOC\n",
    "\n",
    "from typing import Callable, Optional\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "############# Stuff ## ###############################################\n",
    "TRAINING = False # Set false if you only want to evaluate\n",
    "ROOT = 'data/VOCdevkit/VOC2012/'\n",
    "######################################################################\n",
    "\n",
    "class dataset_voc(Dataset):\n",
    "    def __init__(self, root_dir, trvaltest, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): path to main file\n",
    "            trvaltest (int): train=0, val=1, test=2\n",
    "            transform (torchvision.transforms): transforms to be applied to data\n",
    "        \"\"\"\n",
    "        if trvaltest == 0:\n",
    "            dataset = 'train'\n",
    "        elif trvaltest == 1:\n",
    "            dataset = 'val'\n",
    "        elif trvaltest == 2:\n",
    "            dataset = 'test'\n",
    "        else:\n",
    "            raise Exception(\"Strange...\")\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        # Dict that maps file names to labels\n",
    "        self.imgfilenames = {}\n",
    "        self.transform = transform\n",
    "\n",
    "        pv = PascalVOC(self.root_dir)\n",
    "        categories = pv.list_image_sets()\n",
    "\n",
    "        # Add all images to dict\n",
    "        for cat in categories:\n",
    "            for img in pv.imgs_from_category_as_list(cat, dataset):\n",
    "                self.imgfilenames[img] = np.zeros(len(categories))\n",
    "\n",
    "        # Label images\n",
    "        for idx in range(len(categories)):\n",
    "            imgs_from_cat = pv.imgs_from_category_as_list(\n",
    "                categories[idx], dataset)\n",
    "            for img in imgs_from_cat:\n",
    "                # Update labels\n",
    "                self.imgfilenames[img][idx] = 1\n",
    "\n",
    "        # Cast to list, so it is possible to index it\n",
    "        self.imgfilenames = list(self.imgfilenames.items())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgfilenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = PIL.Image.open(\n",
    "            self.root_dir + \"JPEGImages/\"  +  self.imgfilenames[idx][0] + \".jpg\")\n",
    "        image = self.transform(image)\n",
    "        label = self.imgfilenames[idx][1]\n",
    "\n",
    "        sample = {'image': image, 'label': label,\n",
    "                  'filename': self.imgfilenames[idx][0]}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def train_epoch(model,  trainloader,  criterion, device, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    running_loss = 0\n",
    "    for batch_idx, data in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = data['image'].to(device)\n",
    "        labels = data['label'].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if loss.isnan().any():\n",
    "            print(\"NaN found\")\n",
    "            sys.exit()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def evaluate_meanavgprecision(model, dataloader, criterion, device, numcl):\n",
    "    print(\"Evaluating...\")\n",
    "    model.eval()\n",
    "\n",
    "    prediction_labels = [[] for _ in range(numcl)]\n",
    "    prediction_scores = [[] for _ in range(numcl)]\n",
    "\n",
    "    # prediction scores for each class. each numpy array is a list of scores. one score per image\n",
    "    # Appending to python list should be faster no?\n",
    "    prediction_scores = [[] for _ in range(numcl)]\n",
    "    # concat_pred = [np.empty(shape=(0)) for _ in range(numcl)]\n",
    "    # labels scores for each class. each numpy array is a list of labels. one label per image\n",
    "    prediction_labels = [[] for _ in range(numcl)]\n",
    "    # concat_labels = [np.empty(shape=(0)) for _ in range(numcl)]\n",
    "    avgprecs = np.zeros(numcl)  # average precision for each class\n",
    "    fnames = []  # filenames as they come out of the dataloader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "\n",
    "            if (batch_idx % 100 == 0) and (batch_idx >= 100):\n",
    "                print('at val batchindex: ', batch_idx)\n",
    "\n",
    "            inputs = data['image'].to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            labels = data['label'].to(device)\n",
    "\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            cpuout = outputs.to('cpu')\n",
    "            # Collect scores and label to later calulate average precision\n",
    "            for i in range(labels.shape[0]):\n",
    "                # For every output in batch\n",
    "                single_output = cpuout[i]\n",
    "                label = labels[i]\n",
    "                fnames.append(data['filename'][i])\n",
    "                for j in range(numcl):\n",
    "                    # Add prediction score for every class\n",
    "                    prediction_scores[j].append(single_output[j].cpu().numpy())\n",
    "                    prediction_labels[j].append(label[j].cpu().numpy())\n",
    "\n",
    "            \"\"\"this was an accuracy computation\n",
    "            cpuout= outputs.to('cpu')\n",
    "            _, preds = torch.max(cpuout, 1)\n",
    "            labels = labels.float()\n",
    "            corrects = torch.sum(preds == labels.data)\n",
    "            accuracy = accuracy*( curcount/ float(curcount+labels.shape[0]) ) + corrects.float()* ( curcount/ float(curcount+labels.shape[0]) )\n",
    "            curcount += labels.shape[0]\"\"\"\n",
    "    for c in range(numcl):\n",
    "        avgprecs[c] = sklearn.metrics.average_precision_score(y_true=np.array(\n",
    "            prediction_labels[c]), y_score=np.array(prediction_scores[c]))\n",
    "\n",
    "    write_to_file(prediction_scores, fnames)\n",
    "\n",
    "    return avgprecs, np.mean(losses), prediction_labels, prediction_scores, fnames\n",
    "\n",
    "\n",
    "def write_to_file(prediction_scores, fnames):\n",
    "    f = open(\"results.txt\", \"w\")\n",
    "    for i in range(20):\n",
    "        for j in range(len(fnames)):\n",
    "            if prediction_scores[i][j] > 0:\n",
    "                f.write(f\"{fnames[j]} {prediction_scores[i][j]}\\n\")\n",
    "        f.write(\",\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def traineval2_model_nocv(dataloader_train, dataloader_test,  model,  criterion, optimizer, scheduler, num_epochs, device, numcl):\n",
    "    start_time = int(time.time())\n",
    "    best_measure = 0\n",
    "    best_epoch = -1\n",
    "\n",
    "    trainlosses = []\n",
    "    testlosses = []\n",
    "    testperfs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        avgloss = train_epoch(model,  dataloader_train,\n",
    "                              criterion,  device, optimizer)\n",
    "        trainlosses.append(avgloss)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        perfmeasure, testloss, concat_labels, concat_pred, fnames = evaluate_meanavgprecision(\n",
    "            model, dataloader_test, criterion, device, numcl)\n",
    "        testlosses.append(testloss)\n",
    "        testperfs.append(perfmeasure)\n",
    "\n",
    "\n",
    "        print('at epoch: ', epoch, ' classwise perfmeasure ', perfmeasure)\n",
    "\n",
    "        avgperfmeasure = np.mean(perfmeasure)\n",
    "        print('at epoch: ', epoch, ' avgperfmeasure ', avgperfmeasure)\n",
    "\n",
    "\n",
    "        if avgperfmeasure > best_measure:  # higher is better or lower is better?\n",
    "            bestweights = model.state_dict()\n",
    "            # Tracking current best performance measure and epoch\n",
    "            best_measure = avgperfmeasure\n",
    "            best_epoch = epoch\n",
    "            # Saving weights and scores\n",
    "\n",
    "    return best_epoch, best_measure, bestweights, trainlosses, testlosses, testperfs\n",
    "\n",
    "\n",
    "class yourloss(nn.modules.loss._Loss):\n",
    "\n",
    "    def __init__(self, reduction: str = 'mean') -> None:\n",
    "        \"\"\" \n",
    "        Take a binary cross entropy loss independently \n",
    "        for 20 classes and sum/average it, depending on reduction\n",
    "        \"\"\"\n",
    "        super(yourloss, self).__init__()\n",
    "\n",
    "        def sigmoid(logits):\n",
    "            return torch.where(logits >= 0, 1/(1+torch.exp(-logits)),\n",
    "                               torch.exp(logits)/(1+torch.exp(logits)))\n",
    "\n",
    "        def BCELoss(logits, labels):\n",
    "            probabilities = sigmoid(logits)\n",
    "            return -(labels * torch.log(probabilities) + (1-labels) * torch.log(1-probabilities))\n",
    "\n",
    "        self.loss = BCELoss\n",
    "\n",
    "    def forward(self, input_: Tensor, target: Tensor) -> Tensor:\n",
    "        loss = self.loss(input_, target)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def runstuff():\n",
    "    config = dict()\n",
    "\n",
    "    # True #TODO change this to True for training on the cluster, eh\n",
    "    config['use_gpu'] = True\n",
    "    config['lr'] = 0.0005\n",
    "    config['batchsize_train'] = 128\n",
    "    config['batchsize_val'] = 64\n",
    "    config['maxnumepochs'] = 12\n",
    "\n",
    "    config['scheduler_stepsize'] = 2\n",
    "    config['scheduler_factor'] = 0.3\n",
    "\n",
    "    # kind of a dataset property\n",
    "    config['numcl'] = 20\n",
    "\n",
    "    # data augmentations\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # datasets\n",
    "    image_datasets = {}\n",
    "    image_datasets['train'] = dataset_voc(\n",
    "        root_dir=ROOT, trvaltest=0, transform=data_transforms['train'])\n",
    "    image_datasets['val'] = dataset_voc(\n",
    "        root_dir=ROOT, trvaltest=1, transform=data_transforms['val'])\n",
    "    \n",
    "    # dataloaders\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = DataLoader(\n",
    "        image_datasets['train'], config['batchsize_train'], num_workers=0, shuffle=True)\n",
    "    dataloaders['val'] = DataLoader(\n",
    "        image_datasets['val'], config['batchsize_val'], num_workers=0)\n",
    "\n",
    "    # device\n",
    "    if True == config['use_gpu']:\n",
    "        device = torch.device('cuda:0')\n",
    "\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # model\n",
    "    model = models.resnet18(pretrained=True)  # pretrained resnet18\n",
    "    # overwrite last linear layer\n",
    "    model.fc = nn.Linear(model.fc.in_features, 20)\n",
    "    model = model.to(device)\n",
    "\n",
    "    lossfct = yourloss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    someoptimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    # Decay LR by a factor of 0.3 every X epochs\n",
    "    somelr_scheduler = optim.lr_scheduler.StepLR(\n",
    "        someoptimizer, config['scheduler_stepsize'], config['scheduler_factor'])\n",
    "\n",
    "    TRAINING = True\n",
    "    if TRAINING == False:  # Produce stuff that is needed for report\n",
    "        # This loads best model, evaluates, and writes results to file to be used for GUI\n",
    "\n",
    "        perfmeasure, testloss, concat_labels, concat_pred, fnames = evaluate_meanavgprecision(\n",
    "            model, dataloaders['val'], lossfct, device, config['numcl'])\n",
    "\n",
    "        max_t = np.max(np.array(concat_pred), axis=1).min()\n",
    "        t_dist = np.arange(20) * max_t/20\n",
    "        tailacc = np.zeros(20)\n",
    "        for c in range(20):\n",
    "            scores = np.array(concat_pred[c])\n",
    "            labels = np.array(concat_labels[c])\n",
    "            for t_idx in range(len(t_dist)):\n",
    "                tailacc[t_idx] += 1/(scores[scores > t_dist[t_idx]]\n",
    "                                     ).shape[0] * labels[scores > t_dist[t_idx]].sum()\n",
    "\n",
    "        print(f\"AP Scores: {perfmeasure}, mAP: {perfmeasure.mean()}\")\n",
    "        plt.plot(t_dist, tailacc/20)\n",
    "        plt.xlabel(\"t\")\n",
    "        plt.ylabel(\"Tailacc(t)\")\n",
    "        plt.show()\n",
    "\n",
    "    elif TRAINING == True:\n",
    "        best_epoch, best_measure, bestweights, trainlosses, testlosses, testperfs = traineval2_model_nocv(\n",
    "            dataloaders['train'], dataloaders['val'],  model,  lossfct, someoptimizer, somelr_scheduler, num_epochs=config['maxnumepochs'], device=device, numcl=config['numcl'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    runstuff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-original",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
